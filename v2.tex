\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{amsfonts}
\newcommand{\e}[1]{{\mathbb E}\left[ #1 \right]}

\title{Advanced Machine Learning}
\author{Mirko De Vita \\ Piersilvio De Bartolomeis}
\date{Fall Semester 2020}

\begin{document}

\maketitle

\section{Regression and Bias-Variance trade-off}

The goal of regression is to predict the value of one or more continuous \textit{target} variables \textit{t} given the value of a D-dimensional vector \textbf{x} of \textit{input} variables.
\\The simplest form of linear regression models are also linear functions of the input variables.
\\However, we can obtain a much more useful class of functions by taking linear combinations of a fixed set of nonlinear functions of the input variables, known as \textit{basis functions}. Such models are linear functions of the parameters, which gives them simple analytical properties and yet can be nonlinear with respect to the input variables.
\\Given a training data set comprising \textit{N} observations \{\bm{$x_n$}\} where $n = 1,...,N$, together with corresponding target values \{$t_n$\}, the goal is to predict the value of $t$ for a new value of \textbf{x}.
\\In the simplest approach, this can be done by directly constructing an appropriate function \textit{y}(\textbf{x}) whose values for new inputs \textbf{x} constitute the predictions for the corresponding values of \textit{t}.
\\More generally, from a probabilistic perspective, we aim to model the predictive distribution $p(t|\bm{x})$ because this expresses our uncertainty about the value of \textit{t} for each value of \textbf{x}.
\\From this conditional distribution we can make predictions of $t$, for any new value of \textbf{x}, in such a way as to minimize the expected value of a suitably chosen loss function.



\end{document}