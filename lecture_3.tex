\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\eqdef}{:\mathrel{\mathop=}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,dsfont,amssymb}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Advanced Machine Learning
	\hfill Fall 2020} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it  #3 \hfill  #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}

   {\bf Disclaimer}: {\it These notes are adapted from ETH's Advanced Machine Learning Course and the book All Of Statistics, Larry Wasserman, Springer.}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\DeclareMathOperator*{\argmin}{arg\,min} 

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{Density Estimation}{}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Modelling assumptions for regression}

\textbf{Object space}: $O$, measurement/feature space: $\mathcal{F} = \mathbb{R}^d\times\mathbb{R}$\\
\textbf{Data}: $\mathcal{Z} = \{(x_i, y_i) \in \mathbb{R}^d\times\mathbb{R}: 1 \leq i \leq n\}$\\
\textbf{Model}: $Y$ output, $X = (X_0,X_1,...,X_d)$ features with $X_0 = 1$, $\epsilon$ noise with $\E[\epsilon] = 0$
\begin{equation*}
    \boldsymbol{Y} = f(\boldsymbol{X}, \boldsymbol{\theta}) + \epsilon
\end{equation*}
\subsection{Bayesian view}
Both observations (feature vector $\boldsymbol{X}$ and output $\boldsymbol{Y}$) and the parameters $\boldsymbol{\theta}$ of the regression model are random variables.\\
Bayesian treatment of linear regression avoids over-fitting and also leads to automatic methods of determining model complexity using the training data alone. It makes predictions using all possible parameters, weighted by their posterior probability.\\
Since the likelihood function $\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{X}\boldsymbol{\theta}, \sigma^2\mathds{1})$ is the exponential of a quadratic function, the corresponding conjugate prior is defined by $\mathbb{P}(\boldsymbol{\theta}) = \mathcal{N}(0,\Lambda^{-1})$.\\
(N.B Assuming $\Lambda$ and $\sigma^2$ as fixed is a big assumption).\medskip

Now we compute the posterior distribution, which is proportional to the product of the likelihood function and the prior.
\begin{equation*}
    \mathbb{P}(\boldsymbol{\theta}| \boldsymbol{X}, \boldsymbol{Y}) \propto \mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) \mathbb{P}(\boldsymbol{\theta})
\end{equation*}
\begin{equation*}
\begin{aligned}
    \log\mathbb{P}(\boldsymbol{\theta}| \boldsymbol{X}, \boldsymbol{Y}) &\propto -\frac{1}{2}\sigma^2(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\theta})^\intercal(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\theta}) - \frac{1}{2}\boldsymbol{\theta}^\intercal\Lambda\boldsymbol{\theta} + const\\
    &\propto -\frac{1}{2}\boldsymbol{\theta}^\intercal(\sigma^2\boldsymbol{X}^\intercal\boldsymbol{X} + \Lambda)\boldsymbol{\theta} + \boldsymbol{\theta}^\intercal(\sigma^2\boldsymbol{X}^\intercal\boldsymbol{Y}) + const
\end{aligned}
\end{equation*}
By completing the square we note that:

\end{document}