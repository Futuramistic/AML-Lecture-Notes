\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\eqdef}{:\mathrel{\mathop=}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,dsfont,amssymb, cool, cancel}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Advanced Machine Learning
	\hfill Fall 2020} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it  #3 \hfill  #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}

   {\bf Disclaimer}: {\it These notes are adapted from ETH's Advanced Machine Learning Course and the book All Of Statistics, Larry Wasserman, Springer.}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}{Example}[lecnum]
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min} 

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{Density Estimation}{}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Parametric Inference}

We now turn our attention to parametric models, that is, models of the form:
\begin{equation*}
    \mathfrak{F} = \{f(x; \theta) : \theta \in \Theta\}
\end{equation*}
where the $\theta \subset \mathbb{R}^k$ is the parameter space and $\theta = (\theta_1,...,\theta_k)$ is the parameter. The problem of inference then reduces to the problem of estimating the parameter $\theta$.\\
Often, we are only interested in some function $T(\theta)$. For example, if $X \sim \mathcal{N}(\mu, \sigma^2)$ then the parameter is $\theta = (\mu, \sigma)$. If our goal is to estimate $\mu$ then $\mu = T(\theta)$ is called the parameter of interest and $\sigma$ is called a nuisance parameter.

\subsection{Maximum Likelihood}

The most common method for estimating parameters in a parametric model is the maximum likelihood method. Let $X_1,...,X_n$ be IID with pdf $f(x; \theta)$.
\begin{definition}
The \textbf{likelihood function} is defined by:
\begin{equation*}
    \mathcal{L}_n(\theta) = \prod\limits_{i = 1}^{n}f(x_i; \theta)
\end{equation*}
The \textbf{log-likelihood function} is defined by $l_n(\theta) = \log\mathcal{L}_n(\theta)$.
\end{definition}
The likelihood function is just the joint density of the data, except that we treat it is a function of the parameter $\theta$. Thus,  $\mathcal{L}_n(\theta) : \Theta \to [0, \infty)$. The likelihood function is not a density function: in general, it is not true that $\mathcal{L}_n(\theta)$ integrates to 1 (with respect to $\theta$).
\begin{definition}
The \textbf{maximum likelihood estimator} MLE, denoted by $\hat{\theta_n}$, is the value of $\theta$ that maximizes $\mathcal{L}_n(\theta)$.
\end{definition}
The maximum of $l_n(\theta)$ occurs at the same place as the maximum of $\mathcal{L}_n(\theta)$, so maximizing the log-likelihood leads to the same answer as maximizing the likelihood. Often, it is easier to work with the log-likelihood.
\begin{claim}
If we multiply $\mathcal{L}_n(\theta)$ by any positive constant $c$ (not depending on $\theta$) then this will not change the MLE. Hence, we shall often drop constants in the likelihood function.
\end{claim}
\begin{example}
Let $X_1,...,X_n \sim \mathcal{N}(\mu, \sigma^2)$. The parameter is $\theta = (\mu, \sigma)$ and the likelihood function (ignoring some constants) is:
\begin{equation*}
\begin{aligned}
    \mathcal{L}_n(\mu, \sigma) &= \prod\limits_{i}\frac{1}{\sigma}\exp{\{-\frac{1}{2\sigma^2}(X_i - \mu)^2\}}\\
    &= \sigma^{-n}\exp{\{-\frac{1}{2\sigma^2}\sum\limits_i(X_i - \mu)^2\}}\\
    &= \sigma^{-n}\exp{\{-\frac{nS^2}{2\sigma^2}\}}\exp{\{-\frac{n(\bar{X} - \mu)^2}{2\sigma^2}\}}
\end{aligned}
\end{equation*}
where $\bar{X} = n^{-1}\sum\limits_i X_i$ is the sample mean and $S^2 = n^{-1}\sum\limits_i(X_i - \bar{X})^2$ .The
last equality above follows from the fact that $\sum\limits_i(X_i - \mu)^2 = nS^2 + n(\bar{X} - \mu)^2$
which can be verified by writing $\sum\limits_i(X_i - \mu)^2 = \sum\limits_i(X_i - \bar{X} + \bar{X} -\mu)^2$ and then expanding the square. The log-likelihood is:
\begin{equation*}
    l(\mu, \sigma) = -n\log\sigma - \frac{nS^2}{2\sigma^2} - \frac{n(\bar{X} - \mu)^2}{2\sigma^2}
\end{equation*}
Solving the equations:
\begin{equation*}
\frac{\partial l(\mu, \sigma)}{\partial\mu} \hspace{10pt} \text{and} \hspace{10pt} \frac{\partial l(\mu, \sigma)}{\partial\sigma},
\end{equation*}
we conclude that $\hat{\mu} = \bar{X}$ and $\hat{\sigma} = S$. It can be verified that these are indeed global maxima of the likelihood.
\end{example}
\subsection{Properties of Maximum Likelihood Estimators}
Under certain conditions on the model, the maximum likelihood estimator $\hat{\theta_n}$ possesses many properties that make it an appealing choice of estimator. The main properties of the MLE are:
\begin{enumerate}
    \item The MLE is \textbf{consistent}: $\hat{\theta_n} \overset{P}{\to} \theta^*$ where $\theta^*$ denotes the true value of the parameter $\theta$;
    \item The MLE is \textbf{equivariant}: if $\hat{\theta_n}$ is the MLE of $\theta$ then $g(\hat{\theta_n})$ is the MLE of $g(\theta)$;
    \item  The MLE is \textbf{asymptotically Normal}: $(\hat{\theta} - \theta^* / \hat{\text{se}}) \sim \mathcal{N}(0, 1)$; also, the
estimated standard error $\hat{\text{se}}$ can often be computed analytically;
    \item The MLE is \textbf{asymptotically optimal} or \textbf{efficient}: roughly, this means that among all well-behaved estimators, the MLE has the smallest variance, at least for large samples;
    \item The MLE is approximately the Bayes estimator. 
\end{enumerate}
The properties we discuss only hold if the model satisfies certain regularity conditions. These are essentially smoothness conditions on $f(x; \theta)$ and we assume that they hold.
\end{document}