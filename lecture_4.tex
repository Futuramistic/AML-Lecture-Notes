\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\eqdef}{:\mathrel{\mathop=}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,dsfont,amssymb}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Advanced Machine Learning
	\hfill Fall 2020} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it  #3 \hfill  #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}

   {\bf Disclaimer}: {\it These notes are adapted from ETH's Advanced Machine Learning Course and Linear Regression via Maximization of the Likelihood,COS 234, Princeton.}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min} 

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{4}{Regression, bias-variance tradeoff}{}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Modelling assumptions for regression}

\textbf{Object space}: $O$, measurement/feature space: $\mathcal{F} = \mathbb{R}^d\times\mathbb{R}$\\
\textbf{Data}: $\mathcal{Z} = \{(x_i, y_i) \in \mathbb{R}^d\times\mathbb{R}: 1 \leq i \leq n\}$\\
\textbf{Model}: $Y$ output, $X = (X_0,X_1,...,X_d)$ features with $X_0 = 1$, $\epsilon$ noise with $\E[\epsilon] = 0$
\begin{equation*}
    \boldsymbol{Y} = f(\boldsymbol{X}, \boldsymbol{\theta}) + \epsilon
\end{equation*}
\section{MLE Regression with Gaussian Noise}
Assumption: noise comes from a zero-mean Gaussian distribution with variance $\sigma^2$, i.e $\epsilon \sim \mathcal{N}(0, \sigma^2)$.\\
Adding a constant to a Gaussian just has the effect of shifting its mean, so the resulting conditional
probability distribution for our generative probabilistic process is:
\begin{equation*}
    \mathbb{P}(\boldsymbol{y_i} | \: \boldsymbol{x_i}, \boldsymbol{\theta}, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\exp{\left \{-\frac{1}{2\sigma^2}(\boldsymbol{y_i} - \boldsymbol{x_i}^{\intercal}\boldsymbol{\theta})\right\}}
\end{equation*}
We denote the noise associated with the $i$th observation as $\epsilon_i$ and we will take these to be independent and identically distributed. This allows us to write the overall likelihood function as a product over these N terms:
\begin{equation*}
    \mathbb{P}(\{\boldsymbol{y_i}\}_{i = 1}^N | \: \{\boldsymbol{x_i}\}_{i = 1}^N, \boldsymbol{\theta}, \sigma^2) = \prod\limits_{i = 1}^N\frac{1}{\sigma\sqrt{2\pi}}\exp{\left \{-\frac{1}{2\sigma^2}(\boldsymbol{y_i} - \boldsymbol{x_i}^{\intercal}\boldsymbol{\theta})\right\}}
\end{equation*}
Now we are going to turn this univariate Gaussian distribution into a multivariate Gaussian distribution with a diagonal covariance matrix:
\begin{equation*}
    \mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{X}\boldsymbol{\theta}, \sigma^2\mathds{1}) = (2\sigma^2\pi)^{-N / 2}\exp{\left\{ -\frac{1}{2\sigma^2}(\boldsymbol{X\theta} - \boldsymbol{Y})^{\intercal}(\boldsymbol{X\theta} - \boldsymbol{Y}) \right\}}
\end{equation*}
We can now think about how we should maximize this with respect to $\boldsymbol{\theta}$ in order to find the maximum
likelihood estimate. Thus, it is helpful to take the natural log first:
\begin{equation*}
    \log\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) = -\frac{N}{2}\log(2\sigma^2\pi) -\frac{1}{2\sigma^2}(\boldsymbol{X\theta} - \boldsymbol{Y})^{\intercal}(\boldsymbol{X\theta} - \boldsymbol{Y})
\end{equation*}
The additive term does not have a $\boldsymbol{\theta}$. We are then left with the following optimization problem:
\begin{equation*}
    \boldsymbol{\theta^{\text{MLE}}} = \argmax_{\theta}\left\{ -\frac{1}{2\sigma^2}(\boldsymbol{X\theta} - \boldsymbol{Y})^{\intercal}(\boldsymbol{X\theta} - \boldsymbol{Y}) \right\}
\end{equation*}
The $\frac{1}{2\sigma^2}$ does not change the solution to this problem and of course we could change the sign and make
this maximization into a minimization:
\begin{equation*}
    \boldsymbol{\theta^{\text{MLE}}} = \argmin_{\theta}(\boldsymbol{X\theta} - \boldsymbol{Y})^{\intercal}(\boldsymbol{X\theta} - \boldsymbol{Y})
\end{equation*}
This is exactly the same optimization problem for the least-squares linear regression! While it seems like the loss function view and the maximum likelihood view are different, this reveals that they are often the same under the hood: least squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions.
\subsection{Fitting $\sigma^2$}
One thing that is different about maximum likelihood, however, is that it gives us an additional parameter to play with that helps us reason about the \textit{predictive distribution}. The predictive distribution is the distribution over the label, given parameters we have just fit. Rather than simply producing a single estimate, when we have a probabilistic model we can account for noise when we look at test data.\\
That is, after finding $\boldsymbol{\theta}^{\text{MLE}}$ if we have a query input $\boldsymbol{x_{\text{pred}}}$ for which we do not know the $\boldsymbol{y}$, we could compute a guess via $y_{\text{pred}} = \boldsymbol{x_{\text{pred}}}\boldsymbol{\theta}^{\text{MLE}}$, or we could actually construct a whole distribution:




\section{Bayesian view}
Both observations (feature vector $\boldsymbol{X}$ and output $\boldsymbol{Y}$) and the parameters $\boldsymbol{\theta}$ of the regression model are random variables.\\
Bayesian treatment of linear regression avoids over-fitting and also leads to automatic methods of determining model complexity using the training data alone. It makes predictions using all possible parameters, weighted by their posterior probability.\\
Since the likelihood function $\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{X}\boldsymbol{\theta}, \sigma^2\mathds{1})$ is the exponential of a quadratic function, the corresponding conjugate prior is defined by $\mathbb{P}(\boldsymbol{\theta}) = \mathcal{N}(0,\Lambda^{-1})$.\\
(N.B Assuming $\Lambda$ and $\sigma^2$ as fixed is a big assumption).\medskip

Now we compute the posterior distribution, which is proportional to the product of the likelihood function and the prior.
\begin{equation*}
\begin{aligned}
    \mathbb{P}(\boldsymbol{\theta}| \boldsymbol{X}, \boldsymbol{Y}) &\propto \mathbb{P}(\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\theta}) \mathbb{P}(\boldsymbol{\theta})\\
    \log\mathbb{P}(\boldsymbol{\theta}| \boldsymbol{X}, \boldsymbol{Y}) &\propto -\frac{1}{2}\sigma^2(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\theta})^\intercal(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\theta}) - \frac{1}{2}\boldsymbol{\theta}^\intercal\Lambda\boldsymbol{\theta} + const\\
    &\propto -\frac{1}{2}\boldsymbol{\theta}^\intercal(\sigma^2\boldsymbol{X}^\intercal\boldsymbol{X} + \Lambda)\boldsymbol{\theta} + \boldsymbol{\theta}^\intercal(\sigma^2\boldsymbol{X}^\intercal\boldsymbol{Y}) + const
\end{aligned}
\end{equation*}
By completing the square we note that:
\begin{equation*}
    \Sigma_{\theta} = \sigma^{2}(\boldsymbol{X}^{\intercal}\boldsymbol{X} + \sigma^{2}\Lambda)^{-1}
\end{equation*}
\begin{equation*}
    \mu_{\theta} = \sigma^{-2}\Sigma_{\theta}\boldsymbol{X}^{\intercal}\boldsymbol{y} = (\boldsymbol{X}^{\intercal}\boldsymbol{X} + \sigma^{2}\Lambda)^{-1}\boldsymbol{X}^{\intercal}\boldsymbol{Y}
\end{equation*}
If we consider an infinitely broad prior $\Lambda = \alpha\mathds{1}$ with $\alpha \to 0$, the mean $\mu_\theta$ of the posterior distribution reduces to the maximum likelihood value.\\
Similarly if $N = 0$, then the posterior distribution refers to the prior.\\
Furthermore, if data points arrive sequentially, then the posterior distribution at any stage acts as the prior distribution for the subsequent data point.\\
If we consider a zero-mean isotropic Gaussian governed by a precision parameter $\alpha^{-1}$ so that:
\begin{equation*}
    \mathbb{P}(\boldsymbol{\theta}|\:\alpha) = \mathcal{N}(\boldsymbol{\theta}|\:0,\alpha^{-1}\mathds{1})
\end{equation*}
The corresponding posterior distribution over $\boldsymbol{\theta}$ has:
\begin{equation*}
    \mu_\theta = \sigma^{-2}(\alpha\mathds{1} + \sigma^{-2}\boldsymbol{X}^{\intercal}\boldsymbol{X})^{-1}\boldsymbol{X}^{\intercal}\boldsymbol{Y} = (\alpha\sigma^2\mathds{1} + \boldsymbol{X}^{\intercal}\boldsymbol{X})\boldsymbol{X}^{\intercal}\boldsymbol{Y}
\end{equation*}
\begin{equation*}
    \Sigma_\theta^{-1} = \alpha\mathds{1} + \sigma^{-2}\boldsymbol{X}^{\intercal}\boldsymbol{X}
\end{equation*}
The $\log$ of the posterior distribution is given by the sum of the log-likelihood and the $\log$ of the prior, and as a function of $\boldsymbol{\theta}$, takes the form:
\begin{equation*}
    \log\mathbb{P}(\boldsymbol{\theta}|\:\alpha) = -\frac{\sigma^{-2}}{2}\sum\limits_{i = 1}^{N}(\boldsymbol{y_i} - \boldsymbol{\theta}^{\intercal}\boldsymbol{x_i})^2 - \frac{\alpha}{2}\boldsymbol{\theta}^{\intercal}\boldsymbol{\theta} + const
\end{equation*}
Maximization of this posterior distribution with respect to $\boldsymbol{\theta}$ is therefore equivalent to the minimization of the sum-of-squares error function with the addition of a quadratic regularization term ($\lambda = \alpha\sigma^2$).
\end{document}